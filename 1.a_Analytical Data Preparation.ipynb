{"cells":[{"cell_type":"code","source":["!pip install xlrd==1.2.0\n!pip install openpyxl\n!pip install geopandas\n!pip install geopy"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Installing libraries","showTitle":true,"inputWidgets":{},"nuid":"11da7871-faf6-44b2-9db1-7ea7f8660f34"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Requirement already satisfied: xlrd==1.2.0 in /databricks/python3/lib/python3.8/site-packages (1.2.0)\r\n<span class=\"ansi-yellow-fg\">WARNING: You are using pip version 21.0.1; however, version 22.0.3 is available.\r\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\nRequirement already satisfied: openpyxl in /databricks/python3/lib/python3.8/site-packages (3.0.9)\r\nRequirement already satisfied: et-xmlfile in /databricks/python3/lib/python3.8/site-packages (from openpyxl) (1.1.0)\r\n<span class=\"ansi-yellow-fg\">WARNING: You are using pip version 21.0.1; however, version 22.0.3 is available.\r\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\nRequirement already satisfied: geopandas in /databricks/python3/lib/python3.8/site-packages (0.10.2)\r\nRequirement already satisfied: fiona&gt;=1.8 in /databricks/python3/lib/python3.8/site-packages (from geopandas) (1.8.21)\r\nRequirement already satisfied: shapely&gt;=1.6 in /databricks/python3/lib/python3.8/site-packages (from geopandas) (1.8.1.post1)\r\nRequirement already satisfied: pyproj&gt;=2.2.0 in /databricks/python3/lib/python3.8/site-packages (from geopandas) (3.3.0)\r\nRequirement already satisfied: pandas&gt;=0.25.0 in /databricks/python3/lib/python3.8/site-packages (from geopandas) (1.2.4)\r\nRequirement already satisfied: six&gt;=1.7 in /databricks/python3/lib/python3.8/site-packages (from fiona&gt;=1.8-&gt;geopandas) (1.15.0)\r\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from fiona&gt;=1.8-&gt;geopandas) (52.0.0)\r\nRequirement already satisfied: click&gt;=4.0 in /databricks/python3/lib/python3.8/site-packages (from fiona&gt;=1.8-&gt;geopandas) (8.0.4)\r\nRequirement already satisfied: certifi in /databricks/python3/lib/python3.8/site-packages (from fiona&gt;=1.8-&gt;geopandas) (2020.12.5)\r\nRequirement already satisfied: munch in /databricks/python3/lib/python3.8/site-packages (from fiona&gt;=1.8-&gt;geopandas) (2.5.0)\r\nRequirement already satisfied: cligj&gt;=0.5 in /databricks/python3/lib/python3.8/site-packages (from fiona&gt;=1.8-&gt;geopandas) (0.7.2)\r\nRequirement already satisfied: click-plugins&gt;=1.0 in /databricks/python3/lib/python3.8/site-packages (from fiona&gt;=1.8-&gt;geopandas) (1.1.1)\r\nRequirement already satisfied: attrs&gt;=17 in /databricks/python3/lib/python3.8/site-packages (from fiona&gt;=1.8-&gt;geopandas) (20.3.0)\r\nRequirement already satisfied: pytz&gt;=2017.3 in /databricks/python3/lib/python3.8/site-packages (from pandas&gt;=0.25.0-&gt;geopandas) (2020.5)\r\nRequirement already satisfied: numpy&gt;=1.16.5 in /databricks/python3/lib/python3.8/site-packages (from pandas&gt;=0.25.0-&gt;geopandas) (1.20.1)\r\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /databricks/python3/lib/python3.8/site-packages (from pandas&gt;=0.25.0-&gt;geopandas) (2.8.1)\r\n<span class=\"ansi-yellow-fg\">WARNING: You are using pip version 21.0.1; however, version 22.0.3 is available.\r\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\nRequirement already satisfied: geopy in /databricks/python3/lib/python3.8/site-packages (2.2.0)\r\nRequirement already satisfied: geographiclib&lt;2,&gt;=1.49 in /databricks/python3/lib/python3.8/site-packages (from geopy) (1.52)\r\n<span class=\"ansi-yellow-fg\">WARNING: You are using pip version 21.0.1; however, version 22.0.3 is available.\r\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Requirement already satisfied: xlrd==1.2.0 in /databricks/python3/lib/python3.8/site-packages (1.2.0)\r\n<span class=\"ansi-yellow-fg\">WARNING: You are using pip version 21.0.1; however, version 22.0.3 is available.\r\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\nRequirement already satisfied: openpyxl in /databricks/python3/lib/python3.8/site-packages (3.0.9)\r\nRequirement already satisfied: et-xmlfile in /databricks/python3/lib/python3.8/site-packages (from openpyxl) (1.1.0)\r\n<span class=\"ansi-yellow-fg\">WARNING: You are using pip version 21.0.1; however, version 22.0.3 is available.\r\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\nRequirement already satisfied: geopandas in /databricks/python3/lib/python3.8/site-packages (0.10.2)\r\nRequirement already satisfied: fiona&gt;=1.8 in /databricks/python3/lib/python3.8/site-packages (from geopandas) (1.8.21)\r\nRequirement already satisfied: shapely&gt;=1.6 in /databricks/python3/lib/python3.8/site-packages (from geopandas) (1.8.1.post1)\r\nRequirement already satisfied: pyproj&gt;=2.2.0 in /databricks/python3/lib/python3.8/site-packages (from geopandas) (3.3.0)\r\nRequirement already satisfied: pandas&gt;=0.25.0 in /databricks/python3/lib/python3.8/site-packages (from geopandas) (1.2.4)\r\nRequirement already satisfied: six&gt;=1.7 in /databricks/python3/lib/python3.8/site-packages (from fiona&gt;=1.8-&gt;geopandas) (1.15.0)\r\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from fiona&gt;=1.8-&gt;geopandas) (52.0.0)\r\nRequirement already satisfied: click&gt;=4.0 in /databricks/python3/lib/python3.8/site-packages (from fiona&gt;=1.8-&gt;geopandas) (8.0.4)\r\nRequirement already satisfied: certifi in /databricks/python3/lib/python3.8/site-packages (from fiona&gt;=1.8-&gt;geopandas) (2020.12.5)\r\nRequirement already satisfied: munch in /databricks/python3/lib/python3.8/site-packages (from fiona&gt;=1.8-&gt;geopandas) (2.5.0)\r\nRequirement already satisfied: cligj&gt;=0.5 in /databricks/python3/lib/python3.8/site-packages (from fiona&gt;=1.8-&gt;geopandas) (0.7.2)\r\nRequirement already satisfied: click-plugins&gt;=1.0 in /databricks/python3/lib/python3.8/site-packages (from fiona&gt;=1.8-&gt;geopandas) (1.1.1)\r\nRequirement already satisfied: attrs&gt;=17 in /databricks/python3/lib/python3.8/site-packages (from fiona&gt;=1.8-&gt;geopandas) (20.3.0)\r\nRequirement already satisfied: pytz&gt;=2017.3 in /databricks/python3/lib/python3.8/site-packages (from pandas&gt;=0.25.0-&gt;geopandas) (2020.5)\r\nRequirement already satisfied: numpy&gt;=1.16.5 in /databricks/python3/lib/python3.8/site-packages (from pandas&gt;=0.25.0-&gt;geopandas) (1.20.1)\r\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /databricks/python3/lib/python3.8/site-packages (from pandas&gt;=0.25.0-&gt;geopandas) (2.8.1)\r\n<span class=\"ansi-yellow-fg\">WARNING: You are using pip version 21.0.1; however, version 22.0.3 is available.\r\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\nRequirement already satisfied: geopy in /databricks/python3/lib/python3.8/site-packages (2.2.0)\r\nRequirement already satisfied: geographiclib&lt;2,&gt;=1.49 in /databricks/python3/lib/python3.8/site-packages (from geopy) (1.52)\r\n<span class=\"ansi-yellow-fg\">WARNING: You are using pip version 21.0.1; however, version 22.0.3 is available.\r\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.</span>\r\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\nimport numpy as np\nimport datetime\nfrom datetime import date, timedelta\nimport os\nimport warnings\nimport xlrd\nfrom pandas.tseries.holiday import USFederalHolidayCalendar\nfrom pandas.tseries.offsets import CustomBusinessDay\nimport requests\nimport json\nimport geopandas\nfrom geopy.geocoders import Photon\nfrom numpy import cos, sin, arcsin, sqrt\nfrom math import radians\nimport scipy.stats as stats\nfrom functools import reduce\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql import SparkSession\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Importing libraries","showTitle":true,"inputWidgets":{},"nuid":"d1a7dda1-7144-45fd-a7d3-8f38bb64ee3e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["in_scop_mat='enter the path to 171Material_inScope.csv'     # /dbfs/FileStore/tables/171Material_inScope.csv\n#Reading in-scope material file\nin_scope_material=pd.read_csv(in_scop_mat)\n\nves_flat_file='enter the path to VES_data_final_08_12_v1.csv'\n#Reading the combined files of missed PO and all POs data concatenated in excel\nves_final=pd.read_csv(ves_flat_file)\n\nves_old_flat_file = 'enter the path to VES_data_final_06_12.csv'\n#Reading VES old data to get PO Create Date for 2020 POs\nVES_old_data = pd.read_csv(ves_old_flat_file)\n\nPO_Create_Date_file = 'enter the path to PO_Creation_Date.xlsx'\n#Reading PO_Creation_Date data to extract PO Create Date for 2021 POs\nPO_create_date = pd.read_excel(PO_Create_Date_file, skiprows=3)\n\nplant_city_state_file = 'enter the path to Plant_lat_long.csv'\n#reading file containing Plant City ans State\nplant_city_state = pd.read_csv(plant_city_state_file)\n\n#sheet names ['5004 SOLON', '5878 JONESBORO', '5955 GAFFNEY', '5243 MOSS LANDING', '5959 SPRINGVILLE']\nspend_analy='enter the path to Spend_Analysis_Invoice_Based_with_Vendor_Site_2020throughYTD_v3__1___1_.xlsx'\n#sheet names ['5259 LIttle Chute Pack 2020', '5258 MEDFORD_Pack', '5258 MEDFORD_Raw','5446 MT STERLING Raw', '5446 MT STERLING Pack']\nspend_analy1='enter the path to Spend_Analysis_Invoice_Based_with_Vendor_Site_2020throughYTD_v3__2___1_.xlsx'\n#sheet names Report2 plant 5259\nspend_analy2='enter the path to 5259_Raw__1_.xlsx'\n\nsmi_file='enter the path to SMI_data_bricks_raw_data_csv.xlsx'\n#reading the SMI file\nsmi_1=pd.read_excel(smi_file)\n\nfin_recds='enter the path to Risk_Support_raw_data.xlsx'\n#reading financial records\nfin=pd.read_excel(fin_recds)\n\n#reading inventory file\ninventory_flat_file=\"enter the path to inventory1.csv\"\n#reading MB51 consumption file\nMB51_flat_file = 'enter the path to ROH_Frozen_Plant___ZPCK___2020__1_.csv'\nMB51_data=pd.read_csv(MB51_flat_file)\n\n#Reading QIR data to get missing VS locations from VES\nQIR_file = 'enter the path to QIR_updated_copy.csv'\nQIR_updated_data = pd.read_csv(QIR_file)\n\n#Reading Incoterms data\nincoterms_file = 'enter the path to incoterms_file_25_12.csv'\nincoterms_data = pd.read_csv(incoterms_file)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Reading Files","showTitle":true,"inputWidgets":{},"nuid":"0184d387-17e5-49e5-9443-c7f2d242c16f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["order_freq_data='order_freq_19_01'\nmodel_data_final='model_data_vf_19_01'\nhistorical_invt_raw_data ='Historical_inventory_rawData_169_03_01'\ninventory_consumption_mb51='ConsumptionMB51_ROH_ZPCK_28_12'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Output Table","showTitle":true,"inputWidgets":{},"nuid":"ffcbb35e-2dbe-4d7a-9e18-f274a555f05b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["\n#Dropping records where First Statistical delivery Date is null\nves_final.dropna(subset=['First Statistical delivery Date'], inplace=True)\nVES_old_data = VES_old_data[['Plant', 'Purchase_order_PO_item', 'Material No.', 'PO Create Date']].drop_duplicates()\nVES_old_data = VES_old_data.drop_duplicates(subset=['Plant', 'Purchase_order_PO_item', 'Material No.'], keep='last')\nVES_old_data.rename(columns={'Plant':'Plant ID'}, inplace=True)\n\n#Cleaning PO Create Date data to merge with VES data for populating PO_Create_Date in VES data\nPO_create_date.drop(PO_create_date.columns[0],axis=1,inplace=True)\nPO_create_date.rename(columns={'Plant - Key':'Plant ID', 'Material - Key':'Material No.', \n                               'PO Item Number':'PO Item', 'PO Number':'Purchase Order'}, inplace=True)\n\n#Creating unique key of PO number and PO Item to get PO_Create_Date from VES, any missing values will be taken from Spend Analysis\nPO_create_date['Purchase_order_PO_item'] = PO_create_date['Purchase Order'].astype(str) + '_' + PO_create_date['PO Item'].astype(str)\nPO_create_date = PO_create_date.drop_duplicates(subset=['Purchase_order_PO_item', 'Plant ID', 'Material No.'], keep='last')\nPO_create_date.rename(columns={'PO Item Creation Date':'PO Create Date'}, inplace=True)\n\n#Converting date columns to datetime object\nPO_create_date['PO Create Date'] = pd.to_datetime(PO_create_date['PO Create Date'])\nVES_old_data['PO Create Date'] = pd.to_datetime(VES_old_data['PO Create Date'])\n\n#Creating a single PO Create Date file\nPO_create_date_final = pd.concat([PO_create_date[['Purchase_order_PO_item', 'Plant ID', 'Material No.', 'PO Create Date']], \n                                  VES_old_data[['Purchase_order_PO_item', 'Plant ID', 'Material No.', 'PO Create Date']]], ignore_index=True)\nPO_create_date_final = PO_create_date_final.drop_duplicates()\n\n#Merging PO Create Date file with VES data to Populate VES data with PO Create Date\nves_final_w_PO_create_date = ves_final.merge(PO_create_date_final, on=['Purchase_order_PO_item', 'Plant ID', 'Material No.'], how='left')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Data Preprocessing","showTitle":true,"inputWidgets":{},"nuid":"5faeb7de-589f-422c-b8cc-afbcefbfca28"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#2: Changing columns names to write as a spark table\ncol_names = ['_'.join(col.split(' ')) for col in ves_final_w_PO_create_date.columns.tolist()]\nves_final_w_PO_create_date.columns = col_names\n#Changing datatypes of some columns to string to make them uniform\nves_final_w_PO_create_date['MRP_controller'] = ves_final_w_PO_create_date['MRP_controller'].astype(str)\nves_final_w_PO_create_date['On-Time_Delivery(%)'] = ves_final_w_PO_create_date['On-Time_Delivery(%)'].astype(str)\nves_final_w_PO_create_date['Quantity_Reliability(%)'] = ves_final_w_PO_create_date['Quantity_Reliability(%)'].astype(str)\nves_final_w_PO_create_date['Initial_Quality(%)'] = ves_final_w_PO_create_date['Initial_Quality(%)'].astype(str)\nves_final_w_PO_create_date['Total_Quality(%)'] = ves_final_w_PO_create_date['Total_Quality(%)'].astype(str)\nves_final_w_PO_create_date['VM_Name'] = ves_final_w_PO_create_date['VM_Name'].str.lower()\nves_final_w_PO_create_date['Plant_Name'] = ves_final_w_PO_create_date['Plant_Name'].str.lower()\nves_final_w_PO_create_date['VS_Name'] = ves_final_w_PO_create_date['VS_Name'].str.lower()\nves_final_w_PO_create_date['Material_Group'] = ves_final_w_PO_create_date['Material_Group'].str.lower()\nves_final_w_PO_create_date['Material_Description'] = ves_final_w_PO_create_date['Material_Description'].str.lower()\nves_final_w_PO_create_date.rename(columns={'Manual_Override_(Y/N)':'Manual_Override', 'Under_Delievry_Tolerance(%)':'Under_Delievry_Tolerance_pct', \n                                           'Over_Delivery_Tolerance(%)':'Over_Delivery_Tolerance_pct', \n                                           'Early_Delivery_Tolerance(Days)':'Early_Delivery_Tolerance_Days', \n                                           'Late_Delievry_Tolerance(Days)':'Late_Delievry_Tolerance_Days', \n                                           'On-Time_Delivery(%)':'On-Time_Delivery_pct', 'Quantity_Reliability(%)':'Quantity_Reliability_pct', \n                                           'Initial_Quality(%)':'Initial_Quality_pct', 'Total_Quality(%)':'Total_Quality_pct'}, inplace=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb2c40f4-6d1e-491e-9269-fd74870bcf9e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#3: Cleaning Date columns\n#Renaming Last_Statistical_delivery_Date to Delivery_date\nves_final_w_PO_create_date.rename(columns={'Last_Statistical_delivery_Date':'Delivery_date'}, inplace=True)\n\n#Replacing '#' with nans in Date columns to convert to datetime\nves_final_w_PO_create_date['Delivery_date'].replace('#', inplace=True)#check if all these can be added in 1 line\nves_final_w_PO_create_date['First_GR_Date'].replace('#', inplace=True)\nves_final_w_PO_create_date['Last_GR_Date'].replace('#', inplace=True)\nves_final_w_PO_create_date['First_Statistical_delivery_Date'].replace('#', inplace=True)\nves_final_w_PO_create_date['Delivery_date'] = pd.to_datetime(ves_final_w_PO_create_date['Delivery_date'])\nves_final_w_PO_create_date['First_GR_Date'] = pd.to_datetime(ves_final_w_PO_create_date['First_GR_Date'])\nves_final_w_PO_create_date['Last_GR_Date'] = pd.to_datetime(ves_final_w_PO_create_date['Last_GR_Date'])\nves_final_w_PO_create_date['First_Statistical_delivery_Date'] = pd.to_datetime(ves_final_w_PO_create_date['First_Statistical_delivery_Date'])\nves_final_w_PO_create_date['PO_Create_Date'] = pd.to_datetime(ves_final_w_PO_create_date['PO_Create_Date'])\n\n#Replacing nans with a dummy date for PO Create Date: '1888-01-01'\nves_final_w_PO_create_date['PO_Create_Date'].fillna(pd.to_datetime('1888-01-01'), inplace = True)\n\n#Filtering VES for in scope materials\nves = ves_final_w_PO_create_date[ves_final_w_PO_create_date['Material_No.'].isin(in_scope_material['Material'].unique().tolist())]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Cleaning VES data for further processing","showTitle":false,"inputWidgets":{},"nuid":"dd1c4f4e-cb66-4904-95b8-edad7fde43cc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#1: Vendor Subcontractor location, PO Create Date from Spend Analysis\n#Loop for cleaning the Spend Analysis files\nloc_df = []\nfor file in [spend_analy, spend_analy1, spend_analy2]:\n  print(file)\n  spend_analysis_xls = pd.ExcelFile(file)\n  \n  if file == spend_analy:\n    spend_analysis_sheet_names = ['5004 SOLON', '5878 JONESBORO', '5955 GAFFNEY', '5243 MOSS LANDING', '5959 SPRINGVILLE']\n  elif file == spend_analy1:\n    spend_analysis_sheet_names = ['5259 LIttle Chute Pack 2020', '5258 MEDFORD_Pack', '5258 MEDFORD_Raw', \n                                     '5446 MT STERLING Raw', '5446 MT STERLING Pack']\n  else:\n    spend_analysis_sheet_names = ['Report 2']\n    \n  for sheet in spend_analysis_sheet_names:\n    print(sheet)\n    spend_analysis = pd.read_excel(spend_analysis_xls, sheet, skiprows=3, index_col=0)\n    \n    #Renaming columns to match with VES data\n    spend_analysis.rename(columns={'Plant - Key': 'Plant ID','Material - Key':'Material_No.',\n                                   'PO Number - Key':'Purchase_Order','GR Vendor-Site - Key':'sub_id',\n                                   'PO Item Creation Date':'PO_create_date','PO Item Number':'PO_Item',\n                                   'GR Vendor-Site - CAM: City District (Key)':'sub_loc',\n                                   'GR Vendor-Site - Region (Key)':'sub_reg',\n                                   'Supplying Vendor - CAM: City District (Key)':'VS_location', \n                                   'Supplying Vendor - Region (Key)':'VS_region', \n                                   'Supplying Vendor - Key':'Supplier No.',\n                                   'Batch Vendor-Site - Key':'batch_vs_id',\n                                   'Batch Vendor-Site - CAM: City District (Key)':'batch_loc',\n                                   'Batch Vendor-Site - Region (Key)':'batch_reg'},inplace=True)\n    \n    #Forward filling some columns since they are merged in the raw file\n    spend_analysis['PO_create_date'] = spend_analysis['PO_create_date'].fillna(method='ffill')\n    spend_analysis['PO_Item'] = spend_analysis['PO_Item'].fillna(method='ffill')\n    \n    #Subsetting spend analysis data for required columns\n    vs_loc = spend_analysis[['Plant ID','Purchase_Order','PO_Item','Material_No.','PO_create_date','sub_id','sub_loc','sub_reg','Supplier No.', 'VS_location', 'VS_region', 'Supplying Vendor - Country (Key)','batch_vs_id','batch_loc','batch_reg']].drop_duplicates()\n    \n    #Filtering each sheet data for in scope materials\n    vs_loc = vs_loc[vs_loc['Material_No.'].isin(in_scope_material['Material'].unique().tolist())]\n    loc_df.append(vs_loc)\n\nvs_loc_final = pd.concat(loc_df, ignore_index=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Feature Creation: Distance","showTitle":true,"inputWidgets":{},"nuid":"1a5a819e-6c16-4fc2-9cff-dd18da91c9ee"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Creating fields for Vendor Site (VS) and Vendor locations\nvs_loc_final['updated_VS_ID']=vs_loc_final['sub_id']\nvs_loc_final['updated_VS_location']=vs_loc_final['sub_loc']\nvs_loc_final['updated_VS_region']=vs_loc_final['sub_reg']\n\n#There are 3 available field groups for getting the VS ID and location, Following steps are to iterate through these 3 fields to identify non missing values across the 3 fields, namely: GR, Batch and Supplier in the given order of priority (GR>Batch>Supplier)\n\n#First step: logic for obtaining GR\nvs_id_final=vs_loc_final.copy()\n\n#Removing records which contain invalid characters in VS columns\nvs_id_final.drop(vs_id_final.loc[(vs_id_final['sub_id']=='#')|(vs_id_final['sub_id']=='*')].index,axis=0,inplace=True)\nvs_id_final.drop(vs_id_final[vs_id_final['updated_VS_ID']=='#100921243'].index,axis=0,inplace=True)\nvs_id_final['updated_VS_ID']=vs_id_final['updated_VS_ID'].astype(np.int64)\nvs_id_final.drop(columns=['sub_id','sub_loc','sub_reg','Supplier No.', 'VS_location', 'VS_region', 'batch_vs_id', 'batch_loc', 'batch_reg'], \n                 axis=1,inplace=True)\nvs_id_final.drop_duplicates(inplace=True)\nvs_id_final.rename(columns={'Plant ID':'Plant_ID','PO_create_date':'PO_Create_Date'},inplace=True)\nvs_id_final['PO_Create_Date']=pd.to_datetime(vs_id_final['PO_Create_Date'])\nvs_id_final['PO_Item']=vs_id_final['PO_Item'].astype(np.int64)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"40ee3134-1898-42db-b5a0-2bc36dc5270b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Obtain correct PO date from Spend Analysis for records with 1888-01-01\npodate=[]\nfor pod in ves['Purchase_Order'][ves['PO_Create_Date']=='1888-01-01']:\n  a=vs_loc_final[vs_loc_final['Purchase_Order']==pod].reset_index()\n  if a['PO_create_date'].shape[0]>=1:\n    podate.append(a['PO_create_date'][0])\n  else:\n    podate.append('1888-01-01')\nves['PO_Create_Date'][ves['PO_Create_Date']=='1888-01-01']=podate\nves['PO_Create_Date']=pd.to_datetime(ves['PO_Create_Date'])\n\n#Merging GR iteration with VES data\nves_f=pd.merge(ves,vs_id_final,on=('Plant_ID','Purchase_Order','PO_Item','Material_No.','PO_Create_Date'),how='left')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c37c128-4665-4de4-bb2f-8f7296e69b70"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Splitting w.r.t Supplier & Batch for VS_id, VS_location, VS_region\nvs_loc_final_v1=vs_loc_final[(vs_loc_final['updated_VS_ID']=='#') | (vs_loc_final['updated_VS_ID']=='*')]\nvs_loc_final_sup=vs_loc_final_v1[['Plant ID','Purchase_Order','PO_Item','Material_No.','PO_create_date','Supplier No.','VS_location','VS_region','Supplying Vendor - Country (Key)']]\nvs_loc_final_bat=vs_loc_final_v1[['Plant ID', 'Purchase_Order', 'PO_Item', 'Material_No.', 'PO_create_date', 'batch_vs_id', 'batch_loc', \n                                  'batch_reg', 'Supplying Vendor - Country (Key)']]\nvs_loc_final_sup.drop_duplicates(inplace=True)\nvs_loc_final_bat.drop_duplicates(inplace=True)\nvs_loc_final_bat.drop(vs_loc_final_bat[vs_loc_final_bat['batch_vs_id']=='#'].index,axis=0,inplace=True)\nvs_loc_final_bat.drop(vs_loc_final_bat[(vs_loc_final_bat['batch_vs_id']=='MISSING') | \n                                       (vs_loc_final_bat['batch_vs_id']==' *')].index,axis=0,inplace=True)\nvs_loc_final_bat['batch_vs_id']=vs_loc_final_bat['batch_vs_id'].astype(int)\nvs_loc_final_sup['PO_Item']=vs_loc_final_sup['PO_Item'].astype(np.int64)\nvs_loc_final_bat['PO_Item']=vs_loc_final_bat['PO_Item'].astype(np.int64)\nvs_loc_final_sup['PO_create_date']=pd.to_datetime(vs_loc_final_sup['PO_create_date'])\nvs_loc_final_bat['PO_create_date']=pd.to_datetime(vs_loc_final_bat['PO_create_date'])\nvs_loc_final_sup_v1=vs_loc_final_sup[['Supplier No.','VS_location','VS_region','Supplying Vendor - Country (Key)']].copy()\nvs_loc_final_sup_v1=vs_loc_final_sup_v1.drop_duplicates()\nvs_loc_final_sup_v1.rename(columns={'Supplier No.':'updated_VS_ID'},inplace=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ea218174-22f8-493a-840a-76de4ce64eaf"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Second and third step: Logic for obtaining Batch and then Supplier\n#Creating VS info from Batch VS info, if Batch not available then Supplier info is taken, if Supplier not available then filling as null\nves_f_v1=ves_f[ves_f['updated_VS_ID'].isnull()]\nvs_id=[]\nloc=[]\nreg=[]\ncoun=[]\nfor p_id,po,mn,pi,pds,sn in zip(ves_f_v1['Plant_ID'],ves_f_v1['Purchase_Order'],ves_f_v1['Material_No.'],ves_f_v1['PO_Item'],ves_f_v1['PO_Create_Date'],ves_f_v1['Supplier_No.']):\n  a=vs_loc_final_bat[(vs_loc_final_bat['Plant ID']==p_id) & (vs_loc_final_bat['Purchase_Order']==po) & (vs_loc_final_bat['Material_No.']==mn) & (vs_loc_final_bat['PO_Item']==pi) & (vs_loc_final_bat['PO_create_date']==pds)].reset_index()\n  b=vs_loc_final_sup[(vs_loc_final_sup['Plant ID']==p_id) & (vs_loc_final_sup['Purchase_Order']==po) & (vs_loc_final_sup['Material_No.']==mn) & (vs_loc_final_sup['PO_Item']==pi) & (vs_loc_final_sup['PO_create_date']==pds)].reset_index()\n  if a['batch_vs_id'].shape[0]>=1:\n    vs_id.append(a['batch_vs_id'][0])\n    loc.append(a['batch_loc'][0])\n    reg.append(a['batch_reg'][0])\n    coun.append(a['Supplying Vendor - Country (Key)'][0])\n  elif b['Supplier No.'].shape[0]>=1:\n    vs_id.append(b['Supplier No.'][0])\n    loc.append(b['VS_location'][0])\n    reg.append(b['VS_region'][0])\n    coun.append(b['Supplying Vendor - Country (Key)'][0])\n  else:\n    vs_id.append('null')\n    loc.append('null')\n    reg.append('null')\n    coun.append('null')\n\n#Updating VS info created above in VES \nves_f_v1['updated_VS_ID']=vs_id\nves_f_v1['updated_VS_location']=loc\nves_f_v1['updated_VS_region']=reg\nves_f_v1['Supplying Vendor - Country (Key)']=coun"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5f7d905-921a-4d5f-bdc0-13e8cef316ce"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#updating VS ID with null substituted with Supplier no from ves\nves_f_v2=ves_f_v1[ves_f_v1['updated_VS_ID']=='null']\nves_f_v2.loc[(ves_f_v2['updated_VS_ID']=='null'),'updated_VS_ID']=ves_f_v2['Supplier_No.']\n\n#If after all the iterations, VS location is not available, then taking VS location from Spend Analysis Supplier locations\nves_f_sup=pd.merge(ves_f_v2,vs_loc_final_sup_v1,how='left',on=['updated_VS_ID'])\nves_f_sup.drop(columns=['updated_VS_location','updated_VS_region','Supplying Vendor - Country (Key)_x'],axis=0,inplace=True)\nves_f_sup.rename(columns={'VS_location':'updated_VS_location','VS_region':'updated_VS_region','Supplying Vendor - Country (Key)_y':'Supplying Vendor - Country (Key)'},inplace=True)\n\n#Creating cuts for batch, supplier and GR from Spend Analysis\nves_f_batch_sup=ves_f_v1[ves_f_v1['updated_VS_ID']!='null']\nves_f_gr=ves_f[ves_f['updated_VS_ID'].notnull()]\n\n#Creating final VES with updated VS Info\nves_finale=pd.concat([ves_f_gr,ves_f_batch_sup,ves_f_sup],axis=0)\ncols=['_'.join(i.split(' ')) for i in ves_finale.columns.tolist()]\nves_finale.columns = cols\nves_finale.rename(columns={'Supplying_Vendor_-_Country_(Key)':'updated_VS_country'},inplace=True)\n\n#Dropping records with VS ID equal to 0 or 7\nves_finale.drop(ves_finale[(ves_finale['updated_VS_ID']==0)|(ves_finale['updated_VS_ID']==7)].index,axis=0,inplace=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a0522a3a-585c-4127-8c74-f8c5c0a9d064"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Splitting PO Qty and Unit of Measurement\nves_finale[['Purchase_Order_Scheduled_Qty','Purchase_Order_Scheduled_Qty_UoM']]=ves_finale.Purchase_Order_Scheduled_Qty.str.split(' ',expand =True)\nves_finale['updated_VS_ID']=ves_finale['updated_VS_ID'].astype(np.int64)\nves_finale['Purchase_Order_Scheduled_Qty']=ves_finale['Purchase_Order_Scheduled_Qty'].astype(str)\nd=list()\nfor i in ves_finale['Purchase_Order_Scheduled_Qty']:\n  d.append(i.replace(',',''))\nves_finale['Purchase_Order_Scheduled_Qty']=d\nves_finale['Purchase_Order_Scheduled_Qty']=ves_finale['Purchase_Order_Scheduled_Qty'].astype(float)\nves_finale=ves_finale.drop_duplicates()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a7d849fd-c488-46e7-8a6c-25e3937796a2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#2: Math for distance feature creation begins here\n#Converting all character columns to lowercase\nves_finale['updated_VS_location'] = ves_finale['updated_VS_location'].str.lower()\nves_finale['updated_VS_region'] = ves_finale['updated_VS_region'].str.lower()\nves_finale['updated_VS_country'] = ves_finale['updated_VS_country'].str.lower()\nves_finale = ves_finale.reset_index()\nves_finale.drop(columns='index', inplace=True)\n\n#Updating region columns for VS in North America and Outside North America\nves_finale.loc[ves_finale['updated_VS_region'].str.split('/').str[0].isin(['us', 'mx', 'ca']), \n               'updated_VS_region']=ves_finale['updated_VS_region'].str.split('/').str[1]\nves_finale.loc[~ves_finale['updated_VS_region'].str.split('/').str[0].isin(['us', 'mx', 'ca']), \n               'updated_VS_region']=ves_finale['updated_VS_region'].str.split('/').str[0]\n\n# updating some countries and region values in the VES data\nfor vs_id, vs_reg in zip(ves_finale['updated_VS_location'],ves_finale['updated_VS_region']):\n  if (((vs_id=='madison') | (vs_id=='payette')) & (vs_reg=='id')):\n    ves_finale.loc[(ves_finale['updated_VS_location']==vs_id) & (ves_finale['updated_VS_region']==vs_reg),'updated_VS_country']='us'\n  else:\n    pass\n\nves_finale.loc[ves_finale['updated_VS_region']=='de','updated_VS_country']='de'\nves_finale.loc[ves_finale['updated_VS_region']=='br','updated_VS_country']='br'\nves_finale.loc[ves_finale['updated_VS_region']=='vn','updated_VS_country']='vt'\nves_finale.loc[ves_finale['updated_VS_country']=='vt','updated_VS_region']='vt'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f94631a-3068-45ff-97f5-97b5f08aaa85"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#from records where distance is na subsetting records where location is not available\nves_location_na = ves_finale[(ves_finale['updated_VS_location'].isna()==True) | (ves_finale['updated_VS_location']=='#')]\n#Removing records from QIR where VS no is missing\nQIR_updated_data_v1 = QIR_updated_data[QIR_updated_data['VensitnoVS'].isna()==False]\nQIR_updated_data_v1['VensitnoVS'] = QIR_updated_data_v1['VensitnoVS'].astype(int)\n#Getting records from QIR where VS is present in VES VS for missing locations\nves_location_na_v1 = QIR_updated_data_v1[QIR_updated_data_v1['VensitnoVS'].isin(ves_location_na['updated_VS_ID'].unique().tolist())][['VensitnoVS', \n                                                                                                                                 'City(VS)', \n                                                                                                                                 'RG (VS)', 'Country VS']].drop_duplicates()\n\n#Converting all character columns to lowercase\nves_location_na_v1['City(VS)'] = ves_location_na_v1['City(VS)'].str.lower()\nves_location_na_v1['RG (VS)'] = ves_location_na_v1['RG (VS)'].str.lower()\nves_location_na_v1['Country VS'] = ves_location_na_v1['Country VS'].str.lower()\n\n#For VS locations missing in VES, extracting locations from QIR\nfor index, row in ves_location_na_v1.iterrows():\n    ves_finale.loc[ves_finale['updated_VS_ID']==row['VensitnoVS'], 'updated_VS_location'] = row['City(VS)']\n    ves_finale.loc[ves_finale['updated_VS_ID']==row['VensitnoVS'], 'updated_VS_region'] = row['RG (VS)']\n    ves_finale.loc[ves_finale['updated_VS_ID']==row['VensitnoVS'], 'updated_VS_country'] = row['Country VS']"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"822e66dd-660e-437a-b43e-ff90229e3aae"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# function to find the coordinates of a given city \ndef findGeocode(city):   \n    # try and catch is used to overcome the exception thrown by geolocator using geocodertimedout  \n    try:\n        # Specify the user_agent as your app name, it should not be none\n        #Using the Photon function for geocoding\n        geolocator = Photon(user_agent=\"myGeocoder\")\n        return geolocator.geocode(city)\n    except GeocoderTimedOut:\n        return findGeocode(city) "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"819e7d13-d6f5-497a-987f-ecf97dc42414"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Creating VS location data for getting latitude and longitude\nves_data_for_lat_long = ves_finale[['updated_VS_ID', 'updated_VS_location', 'updated_VS_region']].drop_duplicates()\nves_data_for_lat_long_v1 = ves_data_for_lat_long[(ves_data_for_lat_long['updated_VS_location'].isna()==False) & \n                                                 (ves_data_for_lat_long['updated_VS_location']!='#') & \n                                                 (ves_data_for_lat_long['updated_VS_region'].isna()==False) & \n                                                 (ves_data_for_lat_long['updated_VS_region']!='30')]\nves_data_for_lat_long_v1['VS_city_reg'] = ves_data_for_lat_long_v1['updated_VS_location']+', '+ves_data_for_lat_long_v1['updated_VS_region']\n\n#Initiating lists for storing VS latitude and longitude\nlongitude = []\nlatitude = []\n\n#For loop for generating latitude and longitude for each VS\nfor i in (ves_data_for_lat_long_v1['VS_city_reg']):\n  loc = findGeocode(i)\n  if loc != None:\n    # coordinates returned from function is stored into two separate list\n    latitude.append(loc.latitude)\n    longitude.append(loc.longitude)\n\n    # if coordinate for a city not found, insert \"NaN\" indicating missing value \n  else:\n    latitude.append(np.nan)\n    longitude.append(np.nan)\n\n#Adding latitude and longitude to VS location data\nves_data_for_lat_long_v1['vs_lat'] = latitude\nves_data_for_lat_long_v1['vs_lon'] = longitude"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce150949-6480-4a4f-8d4e-873fccaec51d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Creating Plant location data for getting latitude and longitude\nplant_city_state['Plant ID'] = plant_city_state['Destination'].astype(str).str[0:4]\nplant_city_state = plant_city_state[['Plant ID', 'd_CityState']]\nplant_city_state = plant_city_state.drop_duplicates()\nplant_city_state.rename(columns={'Plant ID':'Plant_ID'}, inplace=True)\nplant_city_state['Plant_ID'] = plant_city_state['Plant_ID'].astype(int)\nplant_city_state = plant_city_state[plant_city_state['Plant_ID'].isin(ves_finale['Plant_ID'].unique().tolist())]\n\n#Initiating lists for storing plant latitude and longitude\nplant_latitude = []\nplant_longitude = []\n\n#For loop for generating latitude and longitude for each VS\nfor i in (plant_city_state['d_CityState']):\n  loc = findGeocode(i)\n  if loc != None:\n    # coordinates returned from function is stored into two separate list\n    plant_latitude.append(loc.latitude)\n    plant_longitude.append(loc.longitude)\n       \n    # if coordinate for a city not found, insert \"NaN\" indicating missing value \n  else:\n    plant_latitude.append(np.nan)\n    plant_longitude.append(np.nan)\n    \n#Adding latitude and longitude to Plant location data\nplant_city_state['plant_lat'] = plant_latitude\nplant_city_state['plant_lon'] = plant_longitude"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9f095977-0376-4e06-8cea-8bc75733932c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Merging VS lat long data and plant lat long data with VES data\nves_finale_v1 = ves_finale.merge(ves_data_for_lat_long_v1[['updated_VS_ID', 'vs_lat', 'vs_lon']], on='updated_VS_ID', how='left')\nves_finale_v2 = ves_finale_v1.merge(plant_city_state[['Plant_ID', 'plant_lat', 'plant_lon']], on='Plant_ID', how='left')\n\n#Manually updating some VS latitudes and longitudes for missing values\nves_finale_v2.loc[ves_finale_v2['updated_VS_ID']==100937131, 'vs_lat'] = -34.84967\nves_finale_v2.loc[ves_finale_v2['updated_VS_ID']==100937131, 'vs_lon'] = 150.61093\nves_finale_v2.loc[ves_finale_v2['updated_VS_ID']==100184989, 'vs_lat'] = 38.0333\nves_finale_v2.loc[ves_finale_v2['updated_VS_ID']==100184989, 'vs_lon'] = -1.1167\nves_finale_v2.loc[ves_finale_v2['updated_VS_ID']==101162016, 'vs_lat'] = 10.964112\nves_finale_v2.loc[ves_finale_v2['updated_VS_ID']==101162016, 'vs_lon'] = 106.856461"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a77993b4-4e1f-4dbf-9df6-184bf40c5503"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Creating haversine function for calculating bird distance for VS outside North America\ndef haversine(row):\n    lon1 = row['vs_lon']\n    lat1 = row['vs_lat']\n    lon2 = row['plant_lon']\n    lat2 = row['plant_lat']\n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n    dlon = lon2 - lon1 \n    dlat = lat2 - lat1 \n    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n    c = 2 * arcsin(sqrt(a)) \n    km = 6367 * c\n    return km"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52bbaa99-0877-4135-921c-4bd563dc3ad2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Creating all the unique PLANT VS combos from VES data\nplant_vs_combo_ves = ves_finale_v2[['Plant_ID', 'updated_VS_ID', 'updated_VS_location', 'updated_VS_region', 'updated_VS_country', \n                                    'vs_lat', 'vs_lon', 'plant_lat', 'plant_lon']].drop_duplicates()\n\n#Filtering out records where VS location is not available\nplant_vs_combo_ves = plant_vs_combo_ves[plant_vs_combo_ves['vs_lat'].isna()==False]\n\n#Splitting Plant VS combos between North America and outside North America\ndistance_north_america = plant_vs_combo_ves[plant_vs_combo_ves['updated_VS_country'].isin(['us', 'mx', 'ca'])]\ndistance_file_iternational = plant_vs_combo_ves[~plant_vs_combo_ves['updated_VS_country'].isin(['us', 'mx', 'ca'])]\n\n#Creating distance column with initial values as 0\ndistance_north_america['distance'] = 0\ndistance_north_america = distance_north_america.reset_index()\ndistance_north_america.drop(columns=['index'], inplace=True)\n\n#For VS within North America, using OSRM package to calculate distance between Plant and VS\nfor index, row in distance_north_america.iterrows():\n    print(index)\n    r = requests.get(f\"http://router.project-osrm.org/route/v1/car/{row['vs_lon']},{row['vs_lat']};{row['plant_lon']},{row['plant_lat']}?overview=false\"\"\")\n    routes = json.loads(r.content)\n    if 'routes' not in list(routes.keys()):\n      distance=0\n    else:\n      distance = routes.get('routes')[0].get('distance')\n    print(distance)\n    distance_north_america.iloc[index, distance_north_america.columns.get_loc('distance')] = distance\n\n#Diving distance calculated using OSRM package by 1000 to get distance in KMs\ndistance_north_america['distance'] = distance_north_america['distance']/1000\n\n#Applying haversine formula to get distance between PLant and VS outside north America\ndistance_file_iternational['distance'] = distance_file_iternational.apply(lambda row: haversine(row), axis=1)\n\n#Combining all the distances in a single dataframe\ndistance_v1 = pd.concat([distance_north_america, distance_file_iternational], ignore_index=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12d23cd6-eccf-490b-aa79-2a8bb1b8815e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Manually calculating some distances for missing values\ndistance_v1.loc[(distance_v1['Plant_ID']==5004) & (distance_v1['updated_VS_ID']==100663389), 'distance'] = 1315.9606\ndistance_v1.loc[(distance_v1['Plant_ID']==5878) & (distance_v1['updated_VS_ID']==100663389), 'distance'] = 644.38134\ndistance_v1.loc[(distance_v1['Plant_ID']==5955) & (distance_v1['updated_VS_ID']==100663389), 'distance'] = 1506.9897\ndistance_v1.loc[(distance_v1['Plant_ID']==5959) & (distance_v1['updated_VS_ID']==100663389), 'distance'] = 1759.978598\ndistance_v1.loc[(distance_v1['Plant_ID']==5259) & (distance_v1['updated_VS_ID']==100855588), 'distance'] = 811.75311\ndistance_v1.loc[(distance_v1['Plant_ID']==5004) & (distance_v1['updated_VS_ID']==100358246), 'distance'] = 669.96991\ndistance_v1.loc[(distance_v1['Plant_ID']==5004) & (distance_v1['updated_VS_ID']==100874123), 'distance'] = 979.92956\ndistance_v1.loc[(distance_v1['Plant_ID']==5878) & (distance_v1['updated_VS_ID']==100874123), 'distance'] = 1201.2144\ndistance_v1.loc[(distance_v1['Plant_ID']==5955) & (distance_v1['updated_VS_ID']==100351791), 'distance'] = 237.70011\ndistance_v1.loc[(distance_v1['Plant_ID']==5955) & (distance_v1['updated_VS_ID']==100358246), 'distance'] = 917.64795\ndistance_v1.loc[(distance_v1['Plant_ID']==5955) & (distance_v1['updated_VS_ID']==100874123), 'distance'] = 218.54892\ndistance_v1.loc[(distance_v1['Plant_ID']==5959) & (distance_v1['updated_VS_ID']==100351791), 'distance'] = 3085.91712\ndistance_v1.loc[(distance_v1['Plant_ID']==5959) & (distance_v1['updated_VS_ID']==100358246), 'distance'] = 3488.41405\ndistance_v1.loc[(distance_v1['Plant_ID']==5959) & (distance_v1['updated_VS_ID']==100838018), 'distance'] = 7745.1141066568825\ndistance_v1.loc[(distance_v1['Plant_ID']==5959) & (distance_v1['updated_VS_ID']==100874123), 'distance'] = 3443.03055\n\n#Subsetting Plant_ID, VS ID and distance columns to merge with VES data\ndistance_v1 = distance_v1[['Plant_ID', 'updated_VS_ID', 'distance']].drop_duplicates()\n\n#Merging distance data with VES data on Plant and VS ID\nves_finale_v3 = ves_finale_v2.merge(distance_v1[['Plant_ID', 'updated_VS_ID', 'distance']], on=['Plant_ID', 'updated_VS_ID'], how='left')\n\n#Creating domestic/international flag\nves_finale_v3.loc[ves_finale_v3['updated_VS_country']=='us', 'dom_or_int'] = 'dom'\nves_finale_v3.loc[ves_finale_v3['updated_VS_country']!='us', 'dom_or_int'] = 'int'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1e92d00-286c-4927-b4b3-b760e5f0bbd7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["incoterms_file"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f2417322-f574-487a-92e7-bed08d7cefcc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["incoterms_data = incoterms_data[incoterms_data['Header Incoterms']=='EXW']\nincoterms_data.rename(columns={'Header Incoterms':'header_incoterms'}, inplace=True)\nincoterms_data = incoterms_data.drop_duplicates()\nves_finale_v4 = ves_finale_v3.merge(incoterms_data, on=['Plant_ID', 'Vendor', 'Material_No.'], how='left')\nves_finale_v4.loc[ves_finale_v4['header_incoterms']=='EXW', 'nestle_managed_freight'] = 'yes'\nves_finale_v4.loc[ves_finale_v4['header_incoterms'].isna()==True, 'nestle_managed_freight'] = 'no'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Feature Creation: nestle_managed_freight","showTitle":true,"inputWidgets":{},"nuid":"91656ffc-efff-4948-b9af-701183d1a7c6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["ves_finale_v4['Total_ordered_QTY'] = ves_finale_v4.groupby(by=['Plant_ID','Material_No.'])['Purchase_Order_Scheduled_Qty'].transform(np.sum)\nves_finale_v4['Total_Purchase_ordered_QTY'] = ves_finale_v4.groupby(by=['Plant_ID','Material_No.','updated_VS_ID'])['Purchase_Order_Scheduled_Qty'].transform(np.sum)\nves_finale_v4['vendor_percentage']=round((ves_finale_v4['Total_Purchase_ordered_QTY']/ves_finale_v4['Total_ordered_QTY'])*100,2)\nmat_count=[]\nfor p_id,vs_id in zip(ves_finale_v4['Plant_ID'],ves_finale_v4['updated_VS_ID']):\n  a=ves_finale_v4[(ves_finale_v4['Plant_ID']==p_id) & (ves_finale_v4['updated_VS_ID']==vs_id)]\n  c=len(a['Material_No.'].unique())\n  mat_count.append(c)\nves_finale_v4['vendor_material_cnt_per_plant']=mat_count"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Feature Creation: vendor_percentage, vendor_material_cnt_per_plant","showTitle":true,"inputWidgets":{},"nuid":"56f23f6a-5d26-4562-a21c-e96108d6cda6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["bom1=pd.read_csv('enter the file path to BOM_5004-1.csv')#Put it at the top\nbom2=pd.read_csv('enter the file path to BOM_V1_wo5004_v1.csv')#Put it at the top\nbom = pd.concat([bom1, bom2], ignore_index=True)\nbom.drop(bom[bom['ComponentMaterialNumber'].isnull()].index,axis=0,inplace=True)\nbom['ComponentMaterialNumber']=bom['ComponentMaterialNumber'].astype(np.int64)\nbom=bom[['LocationID','BillOfMaterial','ComponentMaterialNumber']]\nbom.drop_duplicates(inplace=True)\nbom_cont1=pd.pivot_table(bom,values=['BillOfMaterial'], index=['LocationID','ComponentMaterialNumber'],aggfunc='count').reset_index()\nbom_cont1.rename(columns={'LocationID':'Plant_ID','ComponentMaterialNumber':'Material_No.'},inplace=True)\nves_finale_v5=pd.merge(ves_finale_v4,bom_cont1,on=['Plant_ID','Material_No.'],how='left')\nves_finale_v5.drop(ves_finale_v5[ves_finale_v5['BillOfMaterial'].isnull()].index,inplace=True)\nves_finale_v5['sku_type']=['single' if i<=1 else 'multiple' for i in ves_finale_v5['BillOfMaterial']]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Feature Creation: sku_type","showTitle":true,"inputWidgets":{},"nuid":"63f3c742-1362-43fc-ac80-f46a9d25e52d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["ves_finale_v5['mode_of_transport'] = 'road'\nves_finale_v5.loc[(ves_finale_v5['Plant_ID'].isin([5258, 5259, 5446])) & \n                             (ves_finale_v5['Material_No.'].isin([22001236, 22002545, 22019000, 43433857])), \n                             'mode_of_transport'] = 'rail'\nves_finale_v5['Material_group_type'] = ves_finale_v5['Material_Group_as_Posted'].str.split('').str[1]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Feature Creation: mode_of_transport, Material_group_type","showTitle":true,"inputWidgets":{},"nuid":"b4820e3c-dc22-4afb-82f0-ecb62ed243e3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["ves_finale_v5['PO_Create_Month'] = ves_finale_v5['PO_Create_Date'].dt.month\nves_finale_v5.loc[ves_finale_v5['vendor_percentage']==100, 'single_or_multisource'] = 'single'\nves_finale_v5.loc[ves_finale_v5['vendor_percentage']!=100, 'single_or_multisource'] = 'multi'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Feature Creation: PO_Create_Month, single_or_multisource","showTitle":true,"inputWidgets":{},"nuid":"673f896e-08e1-4def-bfb1-c11c51d5cd0d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["ves_finale_v5 = pd.get_dummies(ves_finale_v5, columns=['sku_type', 'dom_or_int', 'Material_group_type', 'nestle_managed_freight', \n                                                       'mode_of_transport', 'single_or_multisource'],drop_first=True)\nves_finale_v5 = pd.get_dummies(ves_finale_v5, columns=['PO_Create_Month'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Creating dummies for categorical features","showTitle":true,"inputWidgets":{},"nuid":"dbf4bdfe-8996-43f3-9499-38fae048f079"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["us_bd = CustomBusinessDay(calendar=USFederalHolidayCalendar())\ndef bus_day_count(day1, day2):\n    return(len(pd.date_range(day1, day2, freq=us_bd).tolist()))\n#Applying the function\nves_finale_v5['bus_day_count'] = ves_finale_v5.apply(lambda x: bus_day_count(x['PO_Create_Date'], x['Delivery_date']), axis=1)\n#Calculating total days (lead time including weekends and holidays) from PO Create Date to Delivery Date\nves_finale_v5['total_days'] = (pd.to_datetime(ves_finale_v5['Delivery_date']) - pd.to_datetime(ves_finale_v5['PO_Create_Date'])).dt.days\n#Calculation holiday as difference of from total days and business days\nves_finale_v5['holiday_weekend_count'] = ves_finale_v5['total_days']-ves_finale_v5['bus_day_count']\n\nves_finale_v5.loc[(ves_finale_v5['Quantity_Reliability_pct']=='100') & \n                  (ves_finale_v5['On-Time_Delivery_pct']=='100') & \n                  (ves_finale_v5['Total_Quality_pct']=='100'), 'OTIF'] = 1\nves_finale_v5.loc[ves_finale_v5['OTIF'].isna()==True, 'OTIF'] = 0"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Target Creation: lead_time, OTIF","showTitle":true,"inputWidgets":{},"nuid":"9e2bfc1e-d188-4c30-adc2-6b9a7af97fd0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Renaming total_days to lead_time\nves_finale_v5.rename(columns={'total_days':'lead_time'}, inplace=True)\n\n#Dropping columns not required in the model\nves_finale_v5.drop(columns=['Supplier_No.', 'VS_Name', 'BillOfMaterial', 'vs_lon', 'vs_lat', 'plant_lon', 'plant_lat', 'Total_ordered_QTY', \n                            'Total_Purchase_ordered_QTY', 'On-Time_Delivery_pct', 'Quantity_Reliability_pct', 'Initial_Quality_pct', \n                            'Total_Quality_pct', 'Purchase_Order_Scheduled_Qty_UoM', 'Under_Delievry_Tolerance_pct', 'Over_Delivery_Tolerance_pct', \n                            'Early_Delivery_Tolerance_Days', 'Late_Delievry_Tolerance_Days', 'MRP_controller', 'MRP_', \n                            'Manual_Override', 'Material_Group_as_Posted', 'header_incoterms'], inplace=True)\n\n#Dropping roecords where distance in na because VS location is not available\nves_finale_v5 = ves_finale_v5.dropna(subset=['distance'])\n\n#Dropping roecords where PO_Create_Date is not available\nves_finale_v5 = ves_finale_v5[ves_finale_v5['PO_Create_Date']!='1888-01-01']\n\n#Converting region column to str datatype to write to spark table\nves_finale_v5['updated_VS_region'] = ves_finale_v5['updated_VS_region'].astype(str)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75a0edeb-6815-4a14-8ea2-76471379b258"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#null value treatment\nprint('shape before null treatment: ',smi_1.shape)\nsmi_1.drop(smi_1[smi_1['Vendor Number'].isnull()].index,inplace=True)\nsmi_1['Material Number']=smi_1['Material Number'].astype(np.int64)\nsmi_1['Vendor Number']=smi_1['Vendor Number'].astype(np.int64)\nsmi_1.drop(smi_1[smi_1['Vendor Site'].isnull()].index,inplace=True)\nsmi_1['Vendor Site']=smi_1['Vendor Site'].astype(np.int64)\nsmi_1.rename(columns={'Vendor Number':'Vendor','Vendor Site':'updated_VS_ID','PO Number':'Purchase_Order','Material Number':'Material_No.','Material Order Quantity':'Purchase_Order_Scheduled_Qty'},inplace=True)\nprint('shape after null treatment: ',smi_1.shape)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Cleaning SMI data","showTitle":true,"inputWidgets":{},"nuid":"1a196849-af93-4087-a007-43742ec4ff59"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["ves_f1=ves_finale_v5.copy()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e8dcd23-ba75-4fc7-9475-c0ae09b05df2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#ves & SMI datatypes defined\nves_f1['Vendor']=ves_f1['Vendor'].astype(np.int64)\nves_f1['updated_VS_ID']=ves_f1['updated_VS_ID'].astype(np.int64)\nves_f1['Material_No.']=ves_f1['Material_No.'].astype(np.int64)\nves_f1['Purchase_Order_Scheduled_Qty']=ves_f1['Purchase_Order_Scheduled_Qty'].astype(float)\nsmi_1['Vendor']=smi_1['Vendor'].astype(np.int64)\nsmi_1['updated_VS_ID']=smi_1['updated_VS_ID'].astype(np.int64)\nsmi_1['Material_No.']=smi_1['Material_No.'].astype(np.int64)\nsmi_1['Purchase_Order_Scheduled_Qty']=smi_1['Purchase_Order_Scheduled_Qty'].astype(float)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfcc0dd9-687f-47a0-8d67-6555484b76b6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# checking for same PO and material more than one complaint are registered with different complaint ID.\nsmi_1[(smi_1['Purchase_Order']==4567367243) & (smi_1['Material_No.']==44092721)]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c44120a-23f4-4ca4-8f03-c940adc25c2b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# feature creation for Impact types\na=list()\nfor i in smi_1['Impact to Business']:\n  a.append(i.split(' - ')[0])\n\nsmi_1['Impact types']=a"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Feature creation from SMI","showTitle":true,"inputWidgets":{},"nuid":"8624561e-a665-49c8-a1dd-ad320bfe4abe"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# feature creation for last 3m date & last 12m date\ndate_len=[90,365]\ndate_attr=['l3m_date','l12m_date']\nfor i,j in zip(date_len,date_attr):\n  td = timedelta(i)\n  ves_f1[j]=ves_f1['PO_Create_Date']-td"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a89c76ee-3b04-4f04-8724-f0041a665047"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# feature creation for number of l3m & l12m complaints\ndate_attr=['l3m_date','l12m_date']\nmnth=[3,12]\nfor date_att,mth in zip(date_attr,mnth):\n  d='no.of.complaint_{}_months'.format(mth)\n  col_str=[]\n  for vs_id,mn,end_date,start_date in zip(ves_f1['updated_VS_ID'],ves_f1['Material_No.'],ves_f1['PO_Create_Date'],ves_f1[date_att]):\n    a=smi_1[(smi_1['updated_VS_ID']==vs_id) & (smi_1['Material_No.']==mn) & (smi_1['Incident Date']<=end_date) & (smi_1['Incident Date']>=start_date)]\n    col_str.append(a['updated_VS_ID'].count())\n  ves_f1[d]=col_str"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9467166-d07a-4db9-9d9c-1d74fdb999dc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# feature creation for number of l3m complaints major & minor\nfor types in smi_1['Impact types'].unique().tolist():\n  d='no.of.complaint_3_months_{}'.format(types.lower())\n  col_str=[]\n  for vs_id,mn,end_date,start_date in zip(ves_f1['updated_VS_ID'],ves_f1['Material_No.'],ves_f1['PO_Create_Date'],ves_f1['l3m_date']):\n    a=smi_1[(smi_1['updated_VS_ID']==vs_id) & (smi_1['Material_No.']==mn) & (smi_1['Incident Date']<=end_date) & (smi_1['Incident Date']>=start_date)]\n    b=a[a['Impact types']==types]\n    col_str.append(b['updated_VS_ID'].count())\n    \n  ves_f1[d]=col_str"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e27a90f0-fe0b-45d7-a799-66e602c06eb2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#feature creation for number of l12m complaints major & minor\nfor types in smi_1['Impact types'].unique().tolist():\n  d='no.of.complaint_12_months_{}'.format(types.lower())\n  col_str=[]\n  for vs_id,mn,end_date,start_date in zip(ves_f1['updated_VS_ID'],ves_f1['Material_No.'],ves_f1['PO_Create_Date'],ves_f1['l12m_date']):\n    a=smi_1[(smi_1['updated_VS_ID']==vs_id) & (smi_1['Material_No.']==mn) & (smi_1['Incident Date']<=end_date) & (smi_1['Incident Date']>=start_date)]\n    b=a[a['Impact types']==types]\n    col_str.append(b['updated_VS_ID'].count())\n    \n  ves_f1[d]=col_str"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"672ff2f9-3e1c-4040-b8e6-fa1bfed0398b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#feature creation for l3m complaints open\nl3m_open=[]\nfor vs_id,mn,end_date,start_date in zip(ves_f1['updated_VS_ID'],ves_f1['Material_No.'],ves_f1['PO_Create_Date'],ves_f1['l3m_date']):\n  a=smi_1[(smi_1['updated_VS_ID']==vs_id) & (smi_1['Material_No.']==mn) & (smi_1['Incident Date']<=end_date) & (smi_1['Incident Date']>=start_date)]\n  b=a[a['Status']==\"Open\"]\n  l3m_open.append(b['updated_VS_ID'].count())\n  \nves_f1['no.of.complaint_3_months_open']=l3m_open"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6b56c41-e97f-4ce7-b16e-0c7b63309bda"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#feature creation for number of l3m complaints open major & minor\nfor types in smi_1['Impact types'].unique().tolist():\n  d='no.of.complaint_3_months_open_{}'.format(types.lower())\n  col_str=[]\n  for vs_id,mn,end_date,start_date in zip(ves_f1['updated_VS_ID'],ves_f1['Material_No.'],ves_f1['PO_Create_Date'],ves_f1['l3m_date']):\n    a=smi_1[(smi_1['updated_VS_ID']==vs_id) & (smi_1['Material_No.']==mn) & (smi_1['Incident Date']<=end_date) & (smi_1['Incident Date']>=start_date)]\n    b=a[a['Status']==\"Open\"]\n    c=b[b['Impact types']==types]\n    col_str.append(c['updated_VS_ID'].count())\n    \n  ves_f1[d]=col_str"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f81808ce-8c1e-4c0d-bc72-f6ab1023048d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#feature creation for l12m complaint types\nfor cmp_typ in smi_1['Complaint Types'].unique().tolist():\n  d='no.of.Complaint_type_{}_l12m'.format(cmp_typ)\n  g='Complaint_type_{}_l12m_distinct'.format(cmp_typ)\n  Complaint_type=[]\n  for vs_id,mn,end_date,start_date in zip(ves_f1['updated_VS_ID'],ves_f1['Material_No.'],ves_f1['PO_Create_Date'],ves_f1['l12m_date']):\n    a=smi_1[(smi_1['updated_VS_ID']==vs_id) & (smi_1['Material_No.']==mn) & (smi_1['Incident Date']<=end_date) & (smi_1['Incident Date']>=start_date)]\n    b=a[a['Complaint Types']==cmp_typ]\n    Complaint_type.append(b['updated_VS_ID'].count())\n\n  ves_f1[d]=Complaint_type\n  ves_f1[g]=[1 if i!=0 else 0 for i in ves_f1[d]]\n  \ncols=['_'.join(i.split(' ')) for i in ves_f1.columns.tolist()]\nves_f1.columns = cols"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6a880e5-b5c9-4c1b-b0d7-b0e41eea9445"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["ves_smi=ves_f1.copy()\nves_smi.shape"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99fed84a-c744-4bd8-ac9b-16c57205032b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#null value treatment\nfin.drop(fin[fin['Risk Level'].isnull()].index,inplace=True)\n\n#getting dummies for catg variables\nfin_dat=pd.get_dummies(fin,columns=['Risk Level','Rating'])\nfin_dat.drop(['Supplier'],axis=1,inplace=True)\nfin_dat.rename(columns={'Supplier Number':'Vendor'},inplace=True)\n\n#ves_smi data merged with financials\nves_smi_fin=pd.merge(ves_smi,fin_dat,on=['Vendor'],how='left')\n\n#null treatment\nves_smi_fin.fillna(0,inplace=True)\n\ncols=['_'.join(i.split(' ')) for i in ves_smi_fin.columns.tolist()]\nves_smi_fin.columns = cols"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Feature creation from Financial records","showTitle":true,"inputWidgets":{},"nuid":"cc395f8c-cd7b-49c6-a931-4ad847634968"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#selecting the required fields and sorting the data in ascending order\nves_smi_fin_cut=ves_smi_fin[['Plant_ID','Material_No.','updated_VS_ID','PO_Create_Date']]\nves_smi_fin_cut=ves_smi_fin_cut.sort_values(by=['Plant_ID','Material_No.','updated_VS_ID','PO_Create_Date'])\nves_smi_fin_cut.reset_index(inplace=True)\nves_smi_fin_cut.reset_index(inplace=True)\nves_smi_fin_cut.head(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Feature Creation : Order Frequency Calculation","showTitle":true,"inputWidgets":{},"nuid":"8891ce53-df1c-4616-9c10-7fb3bd5d22d4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# function to obtain the days gap between the consecutive orders\ndef checkPrevious(x):\n  try:\n    if(x.level_0 - 1 >= 0 and x.updated_VS_ID == ves_smi_fin_cut.loc[x.level_0-1, 'updated_VS_ID'] and \n       x.Plant_ID == ves_smi_fin_cut.loc[x.level_0-1,'Plant_ID'] and x['Material_No.'] == ves_smi_fin_cut.loc[x.level_0-1, 'Material_No.']):\n      \n      return (x.PO_Create_Date - ves_smi_fin_cut.loc[x.level_0 -1, 'PO_Create_Date']).days\n    return 0\n  except:\n    print(\"Exception\")\n    return 0\nves_smi_fin_cut['days']= ves_smi_fin_cut.apply(checkPrevious, 1)\nves_smi_fin_cut.head(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc40c203-e10c-4f2a-be6e-f3738095ea6f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# average of po create days difference(order frequency)\ndays_non_zero=ves_smi_fin_cut[ves_smi_fin_cut['days']>0]\ndays_non_zero_avg=pd.pivot_table(days_non_zero, values = 'days', index=['Plant_ID','Material_No.','updated_VS_ID'],aggfunc='mean').reset_index()\ndays_zero=ves_smi_fin_cut[ves_smi_fin_cut['days']==0]\ndays_zero_uni=days_zero[['Plant_ID','Material_No.','updated_VS_ID']].drop_duplicates()\norder_freq=pd.merge(days_zero_uni,days_non_zero_avg, how='left', on = ['Plant_ID','Material_No.','updated_VS_ID'])\norder_freq=order_freq.fillna(0)\norder_freq.head(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f523a576-03c7-4870-a7a0-2c27c7e6c60e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Order Freq Definitions: 0-6 days: Daily;, 7-20 days:Weekly; 21-40 days: Monthly, 41-100 days: Quarterly, 101-200 days: Half Yearly, 0 or more than 200 days: Yearly\n#converting the average order_freq as per the condition and merge with the original table\nconditions = [\n    (order_freq['days'] > 0) & (order_freq['days'] <= 6),\n    (order_freq['days'] > 6) & (order_freq['days'] <= 20),\n    (order_freq['days'] > 20) & (order_freq['days'] <= 40),\n    (order_freq['days'] > 40) & (order_freq['days'] <= 100),\n    (order_freq['days'] > 100) & (order_freq['days'] <= 200),\n    (order_freq['days'] > 200) | (order_freq['days']== 0)\n    ]\n\nvalues = ['Daily', 'Weekly', 'Monthly', 'Quarterly','Half Yearly','Yearly']\norder_freq['Delivery_freq'] = np.select(conditions, values)\nves_smi_fin_freq = pd.merge(ves_smi_fin,order_freq, how='left', on = ['Plant_ID','Material_No.','updated_VS_ID'])\norder_freq.head(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68c7a6d0-34b1-438c-8f10-4e4a964cd491"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Appending the current data with the old order frequency data\n# order_freq_hist = spark.table(order_freq_data)\n# order_freq_hist=order_freq_hist.toPandas()\n# order_freq=order_freq_hist.append(order_freq)\n# order_freq.drop_duplicates(inplace= True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1cb5c599-0d1c-4179-bc58-85e6f7f279a6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#order freqency data save\norder_freq = spark.createDataFrame(order_freq)\norder_freq.write.saveAsTable(order_freq_data,mode = 'overwrite')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8fd3b8b9-9b0e-433f-8092-c9a13203d2c1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["ves_smi_fin_freq[['Delivered_Quantity','Delivered_Quantity_UoM']] = ves_smi_fin_freq.Delivered_Quantity.str.split(' ',expand =True)\nves_smi_fin_freq['Delivered_Quantity']=ves_smi_fin_freq['Delivered_Quantity'].astype(str)\n#removing comma from quantity variable\nd=list()\nfor i in ves_smi_fin_freq['Delivered_Quantity']:\n  d.append(i.replace(',',''))\nves_smi_fin_freq['Delivered_Quantity']=d\nves_smi_fin_freq['Delivered_Quantity']=ves_smi_fin_freq['Delivered_Quantity'].astype(float)\n\n#Classification of UoM\nu=list()\nfor i in ves_smi_fin_freq['Delivered_Quantity_UoM']:\n  if ((i=='FT') or (i=='IN')):\n    u.append('length')\n  elif ((i=='KG') or (i=='LB')):\n    u.append('mass')\n  elif i=='GAL':\n    u.append('volume')\n  else:\n    u.append('unit')\nves_smi_fin_freq['Type_of_Measure']=u\n\n#Conversion kg to LB\nkg=2.205\nmeasure=list()\nU=list()\nfor i,j in zip(ves_smi_fin_freq['Delivered_Quantity_UoM'],ves_smi_fin_freq['Delivered_Quantity']):\n  if i=='KG':\n    measure.append(j*2.205)\n    U.append('LB')\n  else:\n    measure.append(j)\n    U.append(i)\n\nves_smi_fin_freq['Delivered_Quantity']=measure"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"UoM Standardization","showTitle":true,"inputWidgets":{},"nuid":"83c83fb1-cd3c-468c-97e7-cfdefa2fcb16"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#selecting the required fields and sorting the data in ascending order\nves_smi_fin_freq_cut=ves_smi_fin_freq[['Plant_ID','updated_VS_ID','Material_No.','Purchase_Order','PO_Item','PO_Create_Date','Purchase_Order_Scheduled_Qty','Delivery_date','Delivered_Quantity','Delivery_freq']]\nves_smi_fin_freq_cut.sort_values(by=['Plant_ID','Material_No.','updated_VS_ID','Delivery_date'], inplace = True)\nves_smi_fin_freq_uni=ves_smi_fin_freq[['Plant_ID','updated_VS_ID','Material_No.']].drop_duplicates()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Feature Creation : PO Qty to Avg Delivered Qty ratio","showTitle":true,"inputWidgets":{},"nuid":"2c9c99fc-8cfd-434c-b490-5c15d54141f1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#function to calculate moving average of delivered quantity\n#if delivery is daily look at last 5 deliveries, if delivery is weekly look at last 4 deliveries, else look at last 3 deliveries\ndef mov_avg(p_id,vs_id,mat_id): \n  df1=ves_smi_fin_freq_cut[(ves_smi_fin_freq_cut['Plant_ID']==p_id)&(ves_smi_fin_freq_cut['updated_VS_ID']==vs_id)&(ves_smi_fin_freq_cut['Material_No.']==mat_id)]\n  df1.reset_index()\n  df1['MovingAverage'] = 0\n  if (df1['Delivery_freq'].unique()== 'Daily'):\n    if len(df1)==1:\n      df1['MovingAverage']= df1['Delivered_Quantity']\n    elif len(df1)==2:\n      df1.iloc[0, df1.columns.get_loc('MovingAverage')]=df1.iloc[0]['Delivered_Quantity']\n      df1.iloc[1, df1.columns.get_loc('MovingAverage')]=df1.iloc[0:2]['Delivered_Quantity'].mean()\n    elif len(df1)==3:\n      df1['MovingAverage'] = df1.rolling(window=3)['Delivered_Quantity'].mean()\n      df1.iloc[0, df1.columns.get_loc('MovingAverage')]=df1.iloc[0]['Delivered_Quantity']\n      df1.iloc[1, df1.columns.get_loc('MovingAverage')]=df1.iloc[0:2]['Delivered_Quantity'].mean()\n    elif len(df1)==4:\n      df1['MovingAverage'] = df1.rolling(window=4)['Delivered_Quantity'].mean()\n      df1.iloc[0, df1.columns.get_loc('MovingAverage')]=df1.iloc[0]['Delivered_Quantity']\n      df1.iloc[1, df1.columns.get_loc('MovingAverage')]=df1.iloc[0:2]['Delivered_Quantity'].mean() \n      df1.iloc[2, df1.columns.get_loc('MovingAverage')]=df1.iloc[0:3]['Delivered_Quantity'].mean()\n    elif len(df1)>=5:\n      df1['MovingAverage'] = df1.rolling(window=5)['Delivered_Quantity'].mean()\n      df1.iloc[0, df1.columns.get_loc('MovingAverage')]=df1.iloc[0]['Delivered_Quantity']\n      df1.iloc[1, df1.columns.get_loc('MovingAverage')]=df1.iloc[0:2]['Delivered_Quantity'].mean() \n      df1.iloc[2, df1.columns.get_loc('MovingAverage')]=df1.iloc[0:3]['Delivered_Quantity'].mean()\n      df1.iloc[3, df1.columns.get_loc('MovingAverage')]=df1.iloc[0:4]['Delivered_Quantity'].mean()\n    return df1\n  elif df1['Delivery_freq'].unique()== 'Weekly':\n    if len(df1)==1:\n      df1['MovingAverage']= df1['Delivered_Quantity']\n    elif len(df1)==2:\n      df1.iloc[0, df1.columns.get_loc('MovingAverage')]=df1.iloc[0]['Delivered_Quantity']\n      df1.iloc[1, df1.columns.get_loc('MovingAverage')]=df1.iloc[0:2]['Delivered_Quantity'].mean()\n    elif len(df1)>2:\n      df1['MovingAverage'] = df1.rolling(window=4)['Delivered_Quantity'].mean()\n      df1.iloc[0, df1.columns.get_loc('MovingAverage')]=df1.iloc[0]['Delivered_Quantity']\n      df1.iloc[1, df1.columns.get_loc('MovingAverage')]=df1.iloc[0:2]['Delivered_Quantity'].mean() \n      df1.iloc[2, df1.columns.get_loc('MovingAverage')]=df1.iloc[0:3]['Delivered_Quantity'].mean()\n    return df1\n  else:\n    if len(df1)==1:\n      df1['MovingAverage']= df1['Delivered_Quantity']\n    elif len(df1)==2:\n      df1.iloc[0, df1.columns.get_loc('MovingAverage')]=df1.iloc[0]['Delivered_Quantity']\n      df1.iloc[1, df1.columns.get_loc('MovingAverage')]=df1.iloc[0:2]['Delivered_Quantity'].mean()\n    elif len(df1)>2:\n      df1['MovingAverage'] = df1.rolling(window=3)['Delivered_Quantity'].mean()\n      df1.iloc[0, df1.columns.get_loc('MovingAverage')]=df1.iloc[0]['Delivered_Quantity']\n      df1.iloc[1, df1.columns.get_loc('MovingAverage')]=df1.iloc[0:2]['Delivered_Quantity'].mean() \n    return df1  \navg_del_qty=pd.DataFrame()\nfor p_id,vs_id,mat_id in zip(ves_smi_fin_freq_uni['Plant_ID'],ves_smi_fin_freq_uni['updated_VS_ID'],ves_smi_fin_freq_uni['Material_No.']):\n  temp_df= mov_avg(p_id,vs_id,mat_id)\n  avg_del_qty=avg_del_qty.append(temp_df)\navg_del_qty.head(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"01124f59-a040-41ff-a87f-43cc477b4219"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#POqty to Avg delivery qty calculation\navg_del_qty['POqty_to_AvgDelQty'] = avg_del_qty['Purchase_Order_Scheduled_Qty']/avg_del_qty['MovingAverage']"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ede500fe-06bf-4913-b6e5-b6b3faad4795"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# adding the moving average and ratio feature to the final table\nmoving_avg_table=avg_del_qty[['Plant_ID','Material_No.','updated_VS_ID','Purchase_Order','PO_Create_Date','PO_Item','MovingAverage','POqty_to_AvgDelQty']]\nves_smi_fin_freq_avg=pd.merge(ves_smi_fin_freq,moving_avg_table, how='left',on=['Plant_ID','Material_No.','updated_VS_ID','Purchase_Order','PO_Create_Date','PO_Item'])\nves_smi_fin_freq_avg.dropna(subset=['POqty_to_AvgDelQty'])\nves_smi_fin_freq_avg = ves_smi_fin_freq_avg[ves_smi_fin_freq_avg['POqty_to_AvgDelQty']!=np.inf]\nprint(ves_smi_fin_freq_avg.shape,ves_smi_fin_freq.shape)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a517ab9f-5daf-4d0a-b17e-c370e349d1eb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#Defining Distance Bin Information value function (Used as a feature in OTIF model)\nclass AttributeRelevance():\n    def seq_palette(self, n_colors):\n        return sns.cubehelix_palette(n_colors, start=.5, rot=-.75, reverse=True)\n\n    def bulk_iv(self, feats, iv, woe_extremes=False):\n        iv_dict = {}\n        for f in feats:\n            iv_df, iv_value = iv.calculate_iv(f)\n            if woe_extremes:\n                iv_dict[f.feature] = [iv_value, iv_df['woe'].min(), iv_df['woe'].max()]\n                cols = ['iv', 'woe_min', 'woe_max']\n            else:\n                iv_dict[f.feature] = iv_value\n                cols = ['iv']\n        df = pd.DataFrame.from_dict(iv_dict, orient='index', columns=cols)\n        return df\n\n    def bulk_stats(self, feats, s):\n        stats_dict = {}\n        for f in feats:\n            p_value, effect_size = s.calculate_chi(f)\n            stats_dict[f.feature] = [p_value, effect_size]\n        df = pd.DataFrame.from_dict(stats_dict, orient='index', columns=['p-value', 'effect_size'])\n        return df\n\n    def analyze(self, feats, iv, s=None, interpretation=False):\n        df_iv = self.bulk_iv(feats, iv).sort_values(by='iv', ascending=False)\n        if s is not None:\n            df_stats = self.bulk_stats(feats, s)\n            df_iv = df_iv.merge(df_stats, left_index=True, right_index=True)\n        if interpretation:\n            df_iv['iv_interpretation'] = df_iv['iv'].apply(iv.interpretation)\n            if s is not None:\n                df_iv['es_interpretation'] = df_iv['effect_size'].apply(s.interpretation)\n        return df_iv\n\n    def draw_iv(self, feats, iv):\n        df = self.analyze(feats, iv)\n        fig, ax = plt.subplots(figsize=(10, 6))\n        sns.barplot(x=df.index, y='iv', data=df, palette=self.seq_palette(len(feats)))\n        ax.set_title('IV values')\n        plt.xticks(rotation=90)\n        plt.show()\n\n    def draw_woe_extremes(self, feats, iv):\n        df = self.bulk_iv(feats, iv, woe_extremes=True).sort_values(by='iv', ascending=False)\n        fig, ax = plt.subplots(figsize=(10, 6))\n        sns.barplot(x=df.index, y='woe_min', data=df, palette=self.seq_palette(len(feats)))\n        sns.barplot(x=df.index, y='woe_max', data=df, palette=self.seq_palette(len(feats)))\n        ax.axhline(y=0, color='black', linewidth=1)\n        ax.set_title('Range of WOE values')\n        ax.set_ylabel('WOE')\n        plt.xticks(rotation=90)\n        plt.show()\n\n    def draw_woe_multiplot(self, feats, iv):\n        n = len(feats)\n        nrows = int(np.ceil(n/3))\n        fig, ax = plt.subplots(nrows=nrows, ncols=3, figsize=(15, nrows*4))\n        for i in range(n):\n            iv_df, iv_value = iv.calculate_iv(feats[i])\n            sns.barplot(x=feats[i].feature, y='woe', data=iv_df, color='#455872', ax=fig.axes[i])\n\n        for ax in fig.axes:\n            plt.sca(ax)\n            plt.xticks(rotation=50)\n\n        plt.tight_layout()\n        plt.show()\n\nclass Analysis():\n    def seq_palette(self, n_colors):\n        return sns.cubehelix_palette(n_colors, start=.5, rot=-.75, reverse=True)\n\n    def group_by_feature(self, feat):\n        df = feat.df_lite \\\n                            .groupby('bin') \\\n                            .agg({'label': ['count', 'sum']}) \\\n                            .reset_index()\n        df.columns = [feat.feature, 'count', 'good']\n        df['bad'] = df['count'] - df['good']\n        return df\n\nclass StatsSignificance(Analysis):\n    def calculate_chi(self, feat):\n        df = self.group_by_feature(feat)\n        df_chi = np.array(df[['good', 'bad']])\n        n = df['count'].sum()\n\n        chi = stats.chi2_contingency(df_chi)\n        cramers_v = np.sqrt(chi[0] / n)          # assume that k=2 (good, bad)\n        return chi[1], cramers_v\n\n    @staticmethod\n    def interpretation(cramers_v):\n        if cramers_v < 0.1:\n            return 'useless'\n        elif cramers_v < 0.2:\n            return 'weak'\n        elif cramers_v < 0.4:\n            return 'medium'\n        elif cramers_v < 0.6:\n            return 'strong'\n        else:\n            return 'very strong'\n\n    def interpret_chi(self, feat):\n        _, cramers_v = self.calculate_chi(feat)\n        return self.interpretation(cramers_v)\n\n    def print_chi(self, feat):\n        p_value, cramers_v = self.calculate_chi(feat)\n        print('P-value: %0.2f\\nEffect size: %0.2f' % (p_value, cramers_v))\n        print('%s is a %s predictor' % (feat.feature.capitalize(), self.interpretation(cramers_v)))\n\n\nclass IV(Analysis):\n    @staticmethod\n    def __perc_share(df, group_name):\n        return df[group_name] / df[group_name].sum()\n\n    def __calculate_perc_share(self, feat):\n        df = self.group_by_feature(feat)\n        df['perc_good'] = self.__perc_share(df, 'good')\n        df['perc_bad'] = self.__perc_share(df, 'bad')\n        df['perc_diff'] = df['perc_good'] - df['perc_bad']\n        return df\n\n    def __calculate_woe(self, feat):\n        df = self.__calculate_perc_share(feat)\n        df['woe'] = np.log(df['perc_good']/df['perc_bad'])\n        df['woe'] = df['woe'].replace([np.inf, -np.inf], np.nan).fillna(0)\n        return df\n\n    def calculate_iv(self, feat):\n        df = self.__calculate_woe(feat)\n        df['iv'] = df['perc_diff'] * df['woe']\n        return df, df['iv'].sum()\n\n    def draw_woe(self, feat):\n        iv_df, iv_value = self.calculate_iv(feat)\n        fig, ax = plt.subplots(figsize=(10,6))\n        sns.barplot(x=feat.feature, y='woe', data=iv_df, palette=self.seq_palette(len(iv_df.index)))\n        ax.set_title('WOE visualization for: ' + feat.feature)\n        plt.show()\n        plt.show()\n\n    @staticmethod\n    def interpretation(iv):\n        if iv < 0.02:\n            return 'useless'\n        elif iv < 0.1:\n            return 'weak'\n        elif iv < 0.3:\n            return 'medium'\n        elif iv < 0.5:\n            return 'strong'\n        else:\n            return 'suspicious'\n\n    def interpret_iv(self, feat):\n        _, iv = self.calculate_iv(feat)\n        return self.interpretation(iv)\n\n    def print_iv(self, feat):\n        _, iv = self.calculate_iv(feat)\n        print('Information value: %0.2f' % iv)\n        print('%s is a %s predictor' % (feat.feature.capitalize(), self.interpretation(iv)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Feature Creation: Distance Bins (Using Information Value)","showTitle":true,"inputWidgets":{},"nuid":"472991ca-793a-4da6-81c6-0331b1a3cafd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["class CategoricalFeature():\n    def __init__(self, df, feature):\n        self.df = df\n        self.feature = feature\n\n    @property\n    def df_lite(self):\n        df_lite = self.df\n        df_lite['bin'] = df_lite[self.feature].fillna('MISSING')\n        return df_lite[['bin', 'label']]\n\n\nclass ContinuousFeature():\n    def __init__(self, df, feature):\n        self.df = df\n        self.feature = feature\n        self.bin_min_size = int(len(self.df) * 0.05)\n\n    def __generate_bins(self, bins_num):\n        df = self.df[[self.feature, 'label']]\n        df['bin'] = pd.qcut(df[self.feature], bins_num, duplicates='drop') \\\n                    .apply(lambda x: x.left) \\\n                    .astype(float)\n        return df\n\n    def __generate_correct_bins(self, bins_max=10):\n        for bins_num in range(bins_max, 1, -1):\n            df = self.__generate_bins(bins_num)\n            df_grouped = pd.DataFrame(df.groupby('bin') \\\n                                      .agg({self.feature: 'count',\n                                            'label': 'sum'})) \\\n                                      .reset_index()\n            r, p = stats.stats.spearmanr(df_grouped['bin'], df_grouped['label'])\n\n            if (\n#                     abs(r)==1 and                                                        # check if woe for bins are monotonic\n                    df_grouped[self.feature].min() > self.bin_min_size                   # check if bin size is greater than 5%\n                    and not (df_grouped[self.feature] == df_grouped['label']).any()      # check if number of good and bad is not equal to 0\n            ):\n                break\n\n        return df\n\n    @property\n    def df_lite(self):\n        df_lite = self.__generate_correct_bins()\n        df_lite['bin'].fillna('MISSING', inplace=True)\n        return df_lite[['bin', 'label']]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7678bbd7-9a8e-44a6-8c9a-f7211e0990b8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# defining distance bin creation by maximizing the information value\nfeat_distance = ContinuousFeature(ves_smi_fin_freq_avg, 'distance')\niv = IV()\nves_smi_fin_freq_avg['label']=ves_smi_fin_freq_avg['OTIF']\nx=iv.group_by_feature(feat_distance)\nbins=iv.group_by_feature(feat_distance)\niv_df, iv_value = iv.calculate_iv(feat_distance)\nprint(iv_df)\nprint('Information value: ', iv_value)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b2c182c-b948-4dd1-bcfa-28165048645b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#creating the distance bin columns and then dummies for distance bins\nk=list(x.distance)\nk.append(ves_smi_fin_freq_avg['distance'].max())\nb=[round(i,0) for i in k]\nves_smi_fin_freq_avg['range_distance'] = pd.cut(round(ves_smi_fin_freq_avg.distance,0),b, include_lowest=True)\nprint(ves_smi_fin_freq_avg['range_distance'].value_counts())\nves_smi_fin_freq_avg_dist=pd.get_dummies(ves_smi_fin_freq_avg,columns=['range_distance'])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd71f269-2a2d-4430-8a93-6bf4e1b70b88"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#renaming the columns\n#Depending on change in Vendor data with respect to distance, the distance bins can change\n#Renaming the dynamically generated columns\ncol_rename_dict = {}\nfor col in ves_smi_fin_freq_avg_dist.columns.tolist():\n  if 'range_distance_' in col:\n    distance_bin_lower_limit = str(int(np.ceil(float(col.split('_distance_')[1].split(',')[0].split('(')[1]))))\n    distance_bin_upper_limit = str(int(np.floor(float(col.split('_distance_')[1].split(',')[1].split(']')[0]))))\n    new_col_name = 'range_bin_'+distance_bin_lower_limit+'_to_'+distance_bin_upper_limit\n    col_rename_dict[col] = new_col_name\n\nves_smi_fin_freq_avg_dist.rename(columns=col_rename_dict,inplace=True)\nves_smi_fin_freq_avg_dist.drop('label',1,inplace=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8d1e1f0-af7e-4fee-88dc-cba1641ec1f8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#saving the model data as table\nves_smi_fin_freq_avg_dist=pd.get_dummies(ves_smi_fin_freq_avg_dist,columns=['Delivery_freq'])\ncol_names = ['_'.join(col.split(' ')) for col in ves_smi_fin_freq_avg_dist.columns.tolist()]\nves_smi_fin_freq_avg_dist.columns = col_names\nves_smi_fin_freq_avg_dist_tab = spark.createDataFrame(ves_smi_fin_freq_avg_dist)\nves_smi_fin_freq_avg_dist_tab.write.saveAsTable(model_data_final,mode = 'overwrite')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d6523ba-b15b-4d3c-8b30-ca8e328807f6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Appending the current data with the old model data\n# model_data_hist = spark.table(model_data_final)\n# model_data_hist=model_data_hist.toPandas()\n# ves_smi_fin_freq_avg_dist=model_data_hist.append(ves_smi_fin_freq_avg_dist)\n# ves_smi_fin_freq_avg_dist.drop_duplicates(inplace= True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d58d2d81-1967-4814-ab7f-5231ad302cc5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["#reading a new inventory file\ninvt = spark.read.csv(\"enter the file path to historical_5004.csv\", header=\"true\", inferSchema=\"true\")\n#impporting the material in scope\nmat_scope= spark.read.csv('enter the file path to 171Material_inScope.csv', header=\"true\", inferSchema=\"true\")\n#converting into tuple\nmat_tupple = mat_scope.select(\"Material\").rdd.flatMap(lambda x: x).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Historical Inventory raw data update","showTitle":true,"inputWidgets":{},"nuid":"966b8476-a549-4f2c-8455-18a61688867b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["invt_scope.display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ccfa9823-6855-489c-9b85-4f571f543624"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#query to extract data for the materials in scope and saving the final table\ninvt.createOrReplaceTempView(\"my_table\")\nspark = SparkSession.builder.appName('invt').getOrCreate()\nquery = \"select * from my_table where MaterialID  IN {0}\".format(tuple(mat_tupple))\n# query = \"select * from my_table where MaterialID  IN (22014017,22015453,22015528,43524805,43641219,43935306)\"\ninvt_scope =spark.sql(query)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5600105-b9f8-4c2c-bbb0-cd0f2075ff42"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#appending the new inventory data with the old table\ninvt_hist=spark.table(historical_invt_raw_data)\ninvt_scope = invt_scope.union(invt_hist)\ninvt_scope=invt_scope.dropDuplicates()\n\n#Only inventory which is available on or before the Snapshot Date can be used\ninvt_scope=invt_scope.filter(invt_scope.SnapshotDate >= invt_scope.AvailableDate)\ninvt_scope=invt_scope.drop(invt_scope.SpecialStockNumber)\nprint((invt_scope.count(), len(invt_scope.columns)))\ninvt_scope.write.saveAsTable(historical_invt_raw_data,mode = 'overwrite')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4188b5ba-ef2c-42d1-b4a0-b0b88c8cf187"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#importing MB51 data from flat file and selecting the required columns\nMB51_data=pd.read_csv(MB51_flat_file)\nMB51_cut=MB51_data[['Document Date - Key','Posting Date - Key','Plant - Key','Material - Key','Movement Type - Key','Qty in BUoM']]\nMB51_cut.rename(columns={'Document Date - Key': 'Document_Date','Posting Date - Key':'Posting_Date',\n                                 'Plant - Key':'Plant','Material - Key':'Material',\n                                 'Movement Type - Key':'Movement_Type','Qty in BUoM':'Qty_in_BUoM',\n                                },inplace=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"MB51 consumption data update","showTitle":true,"inputWidgets":{},"nuid":"116546d5-8edd-4209-b0a6-461da998f229"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#selcting Goods issue specific movement type data and calculating the consumption\nMB51_cut_1=MB51_cut[(invt_cut['Movement_Type']=='261')|(invt_cut['Movement_Type']=='711')|(invt_cut['Movement_Type']=='ZU1')|(invt_cut['Movement_Type']=='Z11')|(invt_cut['Movement_Type']=='201')|(invt_cut['Movement_Type']=='221')|(invt_cut['Movement_Type']=='717')|(invt_cut['Movement_Type']=='262')|(invt_cut['Movement_Type']=='202')|(invt_cut['Movement_Type']=='ZU2')]\n\nMB51_cut_1=MB51_cut_1.sort_values(by=['Plant','Material','Posting_Date'])\ninvt_consumption=pd.pivot_table(MB51_cut_1, values = 'Qty_in_BUoM', index=['Plant','Material','Posting_Date'],aggfunc='sum').reset_index()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6faceb17-1a30-4b67-8e54-214f9d4fc4d9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#importing data from the saved consumption table\nconsumption_hist=spark.table(inventory_consumption_mb51)\nconsumption_hist=consumption_hist.toPandas()\ninvt_consumption=invt_consumption.append(consumption_hist,ignore_index= True)\ninvt_consumption.drop_duplicates(inplace=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f8429ab6-5bae-4408-af57-d77f9adadbf8"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["invt_consumption=spark.createDataFrame(invt_consumption)\ninvt_consumption.write.saveAsTable(inventory_consumption_mb51, mode = 'overwrite')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e1cc09b-3890-4c7c-8be4-008ab0227e08"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"23e84448-114c-4858-92fc-7affda4a7b23"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"1.a_Analytical Data Preparation","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1541108005200865}},"nbformat":4,"nbformat_minor":0}
